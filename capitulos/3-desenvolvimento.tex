\chapter{Desenvolvimento}
\label{cap:Desenvolvimento}
Nesse capítulo são abordadas as etapas para o desenvolvimento do trabalho, tal como as técnicas utilizadas para atingir o objetivo de compor um dicionário de léxicos da língua portuguesa específico para detecção de discursos de ódio promovido pelos usuários chamados de \textit{haters}. Também é apresentada uma ferramenta que será utilizada como base para as novas funcionalidades e testes com os novos léxicos utilizados. Ao final, também são apresentados os resultados esperados com o desenvolvimento, estimando a eficiência e os avanços da pesquisa realizada. 

\section{Geração de \textit{corpus} para Detectar \textit{Haters}}
\label{sec:geracaohater}
Como parte fundamental da pesquisa, será criado um dicionário com as palavras mais utilizadas pelos usuários denominados \textit{haters} que pregam discursos de ódio contra indivíduos nas redes sociais. Para a geração do \textit{corpus} desejado são necessárias algumas etapas de desenvolvimento como pode ser visto na Figura \ref{fig:fluxodesenvolvimento}. A seguir serão explicadas cada uma das etapas a serem realizadas para a obtenção do novo dicionário.

\begin{figure}[!h]
\centering 
\caption{Fluxo de atividades para obtenção de \textit{corpus} desejado}
\includegraphics[scale=0.48]{imagens/fluxodesenvolvimento.png}
\legend{Fonte: O Autor}
\label{fig:fluxodesenvolvimento}
\end{figure}

\subsection{Coleta de \textit{Tweets}}
\label{subsec:coletatweets}
A plataforma selecionada para a coleta de dados textuais será o \textit{Twitter}, rede social categorizada como \textit{microblog} em que os usuários realizam atualizações constantes de conteúdo, sendo esse conteúdo textual, figuras, vídeos, enquetes entre outros. No contexto da rede social, as postagens dos usuários recebem o nome de \textit{tweet} e tem o limite de $280$ caracteres.

% \footnote{Anteriormente com o limite de $140$ caracteres, dados sobre a atualização em: \url{https://blog.twitter.com/official/en_us/topics/product/2017/tweetingmadeeasier.html}}. 

Para realizar a coleta de dados primeiramente é necessário criar uma conta dentro da \textit{Twitter Developer Platform}\footnote{\url{https://developer.twitter.com/}} informando alguns tópicos básicos sobre o uso dos dados e a finalidade da ferramenta a ser desenvolvida. Após cadastro, a plataforma disponibilizará os \textit{tokens}\footnote{chaves de autenticação amplamente utilizadas em implementações \textit{OAuth}. Em que não é necessário informar usuário e senha de acesso à ferramenta \cite{hardt2012oauth}.} que serão necessários para a construção do programa de leitura de \textit{tweets} (Figura \ref{fig:twitterdeveloperplatform}) bem como um painel para controle de acesso aos dados e renovação das \textit{tokens}.

\begin{figure}[!h]
\centering 
\caption{Tela com \textit{tokens} utilizados na API do \textit{Twitter}}
\includegraphics[scale=0.3]{imagens/twitterdeveloperplatform.png}
\legend{Fonte: (TWITTER, 2018)}
\label{fig:twitterdeveloperplatform}
\end{figure}

A linguagem de programação a ser utilizada para a coleta dos dados será a \textit{Java}, linguagem orientada a objetos e multiplataforma mantida pela \textit{Oracle} que é de fácil implementação e bastante utilizada na comunidade \textit{open source} (de código livre) que mantém vários fóruns, \textit{frameworks} e APIs constantemente atualizados pela comunidade. A API utilizada será a \textit{Twitter4J}\footnote{\url{http://twitter4j.org/en/index.html}}, uma biblioteca não-oficial que permite o acesso aos dados do \textit{Twitter} utilizando a linguagem de programação \textit{Java}, a mesma que será utilizada para a coleta de dados, facilitando o aprendizado da ferramenta bem como a manutenção, se necessário. A \textit{Twitter4J} utiliza autenticação \textit{OAuth}, ou seja, fazendo uso dos \textit{tokens} disponibilizados pelo \textit{Twitter Developer Platform} e não necessitando de autenticação com \textit{e-mail} (ou nome de usuário, e senha) a cada acesso realizado. 

Após a construção do programa de coleta de \textit{tweets}, o próximo passo será encontrar filtros de consulta que irão auxiliar na busca por conteúdos úteis para a geração do \textit{corpus}. O primeiro critério de seleção será a busca por perfis de influenciadores digitais (comumente chamados pela versão inglesa \textit{digital influencers}), que são pessoas que tem grande influência na mídia, tendo vários seguidores e também vários \textit{haters} comentando suas postagens. Nesse primeiro momento os próximos filtros serão manualmente aplicados por um especialista humano que realizará a busca por \textit{tweets} neutros e \textit{tweets} considerados de ódio\footnote{Sendo considerado textos que expressem ideologias onde religião, etnia, deficiências ou gênero são ridicularizados.}. 

A classificação, bem como os totais de postagens a serem coletadas pelo especialista podem ser vistos na Tabela \ref{tab:classificacaotweets}.
Após a coleta, os dados serão inseridos em arquivos binários e estarão prontos para a próxima etapa prevista para a obtenção do dicionário de léxicos para postagens de \textit{haters} na língua portuguesa.
\begin{table}[h!]
  \begin{center}
    \caption{Classificação dos \textit{tweets} coletados por especialista humano}
    \label{tab:classificacaotweets}
    \begin{tabular}{ll} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
    \textbf{Classificação} & \textbf{Quantidade de Tweets}\\
    \hline
    \textit{Tweets} Neutros&500 \textit{Tweets}\\
    \textit{Tweets} de Ódio&500 \textit{Tweets}\\
    \hline
    Total&1.000 \textit{Tweets}\\
    \end{tabular}
  \end{center}
  \legend{Fonte: O Autor}
\end{table}

\subsection{Classificação Manual dos Dados}
\label{subsec:classmanual}
Nessa etapa, com o auxílio de um especialista humano, os textos apurados na etapa de coleta de \textit{tweets} (Sub-sessão \ref{subsec:coletatweets}) passarão por uma classificação manual como pode ser visto na Figura \ref{fig:classificacaotextos}, afim de gerar uma base de dados para posterior aprendizado de máquina e para comparação estatística de resultados. 
Nessa etapa os textos poderão ser classificados entre duas categorias: \textit{neutro} e \textit{hater}.Uma primeira categoria compreenderá textos sem nenhum tipo de violência nem extremismo, podendo compreender emoções tanto positivas como negativas. Já a segunda categoria compreenderá textos invasivos, onde a violência verbal seja praticada contra um indivíduo ou um grupo. Dentro desses textos violentos pretende-se também classificar as violências nas subcategorias: racista, religioso, incapacidade e gênero.  

\begin{figure}[!h]
\centering 
\caption{Diagrama de coleta e classificação dos textos}
\includegraphics[scale=0.50]{imagens/coletatweets.png}
\legend{Fonte: O Autor}
\label{fig:classificacaotextos}
\end{figure}

Com essa etapa pretende-se montar uma base de dados que posteriormente será pré-processada para remoção de ruídos e que irá compor o \textit{dataset} para treinamento dentro do \textit{software Weka}. Por isso, é importante que os textos sigam a seguinte formatação: "classificador, racista (true, false), religioso (true, false), incapacidade (true, false), gênero (false),'texto coletado'" como forma de padronização de dados a serem classificados de forma automática como é exemplificado na Figura \ref{fig:arqanalise}, onde pode ser verificado um primeiro texto onde um discurso de ódio foi detectado e sub categorizado como ódio a um gênero e já no segundo texto nada foi percebido nenhum tipo de discurso de ódio, e portanto, o mesmo foi classificado como neutro.

\begin{figure}[!h]
\centering 
\caption{Exemplo de classificação manual }
\includegraphics[scale=0.60]{imagens/arqanalise.png}
\legend{Fonte: O Autor}
\label{fig:arqanalise}
\end{figure}

\subsection{Pré-processamento dos \textit{Tweets}}
Assim como visto na Sub-sessão \ref{subsec:preproc}, a etapa de pré-processamento é necessária para a remoção de informações desnecessárias, realizando uma padronização das informações contidas nos textos coletados realizando uma otimização considerável no conteúdo a ser avaliado no processo de classificação automática. 

Para a composição do \textit{dataset} serão realizadas as técnicas de: remoção de conteúdo numérico, remoção de URLs, substituição de pontuações repetidas, todas as palavras serão transformadas para letras maiúsculas, remoção de palavras \textit{stopwords} e substituição de palavras alongadas. 

Ao realizar as técnicas citadas anteriormente, será gerado um \textit{dataset} otimizado. O mesmo será utilizado na etapa de classificação e treinamento de máquina para a geração do \textit{corpus} objetivo do trabalho. Posteriormente a mesma rotina de pré-classificação será incorporada em ferramenta desenvolvida citada na Sessão \ref{sec:revisaoferramenta}.

\subsection{Classificação Automática Utilizando o \textit{Weka} e Geração de \textit{corpus}}

Tendo realizado as etapas anteriores de coleta de \textit{tweets}, de classificação manual e de pré-processamento, o próximo passo será a aplicação de algoritmos de \textit{machine learning} para a classificação automática dos dados apurados nas etapas anteriores. A ferramenta utilizada nessa etapa será o \textit{software} chamado \textit{Weka} \footnote{\url{https://www.cs.waikato.ac.nz/ml/weka/}}(Figura \ref{fig:telaincialweka}) que é uma coleção de algoritmos de \textit{machine learning} utilizados para tarefas de mineração de dados, contendo ferramentas para preparação, classificação, clusterização, entre outros, além de ter ferramentas de visualização de resultados. O \textit{Weka} é um \textit{software} \textit{open source} sob a \textit{GNU General Public License}\footnote{\url{https://www.gnu.org/licenses/licenses.html\#GPL}} \cite{weka_2018}.

\begin{figure}[!h]
\centering 
\caption{Tela inicial da ferramenta \textit{Weka}}
\includegraphics[scale=0.30]{imagens/wekatelainicial.png}
\legend{Fonte: \cite{weka_2018}}
\label{fig:telaincialweka}
\end{figure}

\newpage

Essa etapa pretende executar três algoritmos de classificação automática de textos para, a partir de seus resultados, gerar o \textit{corpus} com os léxicos mais utilizados nos discursos de ódio. Um esquema com os passos principais pode ser visto na Figura \ref{fig:wekaclassificacao}. Os três algoritmos a serem utilizados nessa etapa são: \textit{Naive Bayes} (NB), \textit{Bayse net} (BN) e \textit{Support Vector Machine} (SVM) \cite{Muralidharan2012,N18-1095}. 

\begin{figure}[!h]
\centering 
\caption{Diagrama da etapa de classificação com o \textit{software} \textit{Weka}}
\includegraphics[scale=0.52]{imagens/wekaclassificacao.png}
\legend{Fonte: O Autor}
\label{fig:wekaclassificacao}
\end{figure}

\section{Integração com \textit{Software} Existente}
\label{sec:integracaosoftware}
Após a composição do \textit{corpus} com as palavras mais utilizadas em discursos de ódio, a nova etapa será agregar essa nova lista ao \textit{software} mencionado na Sessão \ref{sec:revisaoferramenta} para que o mesmo seja capaz de, além de identificar as seis categorias de emoção (alegria, desgosto, medo, raiva, surpresa e tristeza), identificar palavras impróprias e de possíveis ataques de \textit{phishing}, também identifique comentários de ódio comumente praticados pelos \textit{haters}.
Também será necessária a integração com o \textit{Twitter} por meio da API \textit{Twitter4J} já que o \textit{software} está integrado atualmente com o \textit{Facebook} (Figura \ref{fig:implementacao}). 

\begin{figure}[!h]
\centering 
\caption{Diagrama de integração \textit{software} existente}
\includegraphics[scale=0.6]{imagens/implementacao.png}
\legend{Fonte: O Autor}
\label{fig:implementacao}
\end{figure}
Após a integração mencionada, o trabalho atingirá seu objetivo geral e testes serão realizados para verificação de assertividade nas detecções de discursos de ódio dentre as subcategorias mencionadas na Sub-sessão \ref{subsec:classmanual}.

\section{Resultados Esperados}
Durante o desenvolvimento do trabalho serão realizadas as etapas que compõem a Análise de Sentimentos, aplicadas a um problema prático de detecção de discursos de ódio em textos coletados da rede social \textit{Twitter}.

Após a geração de base de dados rotulada manualmente (por especialista humano), a partir de amostras extraídas da rede mencionada, serão aplicados algoritmos supervisionados de classificação automática de textos. O algoritmo de melhor desempenho irá auxiliar na composição de \textit{corpus} com as palavras mais frequentes em discursos de ódio. Com o \textit{software} existente, integrado com novo \textit{corpus}, pretende-se realizar testes com base de dados de teste, a fim de realizar comparações e verificar a efetividade na detecção de discursos de ódio bem como a subcategoria que o texto compõe (discurso de raça, religião, gênero, incapacidade).